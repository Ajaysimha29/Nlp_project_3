{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en3f47A0YlSL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import (BartTokenizer, BartForConditionalGeneration,\n",
        "                          T5Tokenizer, T5ForConditionalGeneration,\n",
        "                          PegasusTokenizer, PegasusForConditionalGeneration,\n",
        "                          BertTokenizer, BertForSequenceClassification,\n",
        "                          Trainer, TrainingArguments)\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import xgboost as xgb\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load SpaCy's English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Load the WikiText dataset\n",
        "dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
        "\n",
        "# Accessing the dataset directly and converting to a list of texts\n",
        "texts = dataset['test']['text'][:100]  # Load a small subset for demonstration\n",
        "\n",
        "# Split dataset into train, validation, and test sets\n",
        "train_texts, temp_texts = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "val_texts, test_texts = train_test_split(temp_texts, test_size=0.5, random_state=42)\n",
        "\n",
        "# Tokenization and preprocessing functions\n",
        "def preprocess_texts(texts, tokenizer, max_length=512):\n",
        "    inputs = tokenizer(texts, max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    return inputs\n",
        "\n",
        "# Fine-tuning Summarization Models\n",
        "def fine_tune_model(model, tokenizer, train_texts, val_texts, epochs=3):\n",
        "    # Prepare the inputs for the model\n",
        "    train_inputs = preprocess_texts(train_texts, tokenizer)\n",
        "    val_inputs = preprocess_texts(val_texts, tokenizer)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    # Trainer object\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_inputs,\n",
        "        eval_dataset=val_inputs\n",
        "    )\n",
        "\n",
        "    # Fine-tune the model\n",
        "    trainer.train()\n",
        "\n",
        "# Fine-tune BART\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "fine_tune_model(bart_model, bart_tokenizer, train_texts, val_texts)\n",
        "\n",
        "# Fine-tune T5\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "fine_tune_model(t5_model, t5_tokenizer, train_texts, val_texts)\n",
        "\n",
        "# Fine-tune Pegasus\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
        "fine_tune_model(pegasus_model, pegasus_tokenizer, train_texts, val_texts)\n",
        "\n",
        "# Classification fine-tuning\n",
        "def fine_tune_bert_model(train_texts, train_labels, val_texts, val_labels, epochs=3):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "    train_inputs = preprocess_texts(train_texts, tokenizer)\n",
        "    val_inputs = preprocess_texts(val_texts, tokenizer)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_inputs,\n",
        "        eval_dataset=val_inputs\n",
        "    )\n",
        "\n",
        "    # Fine-tune the model\n",
        "    trainer.train()\n",
        "\n",
        "# Dummy labels (you would replace this with actual labels from your dataset)\n",
        "train_labels = np.random.randint(0, 2, size=len(train_texts))\n",
        "val_labels = np.random.randint(0, 2, size=len(val_texts))\n",
        "\n",
        "# Fine-tune BERT for classification\n",
        "fine_tune_bert_model(train_texts, train_labels, val_texts, val_labels)\n",
        "\n",
        "# Summarization functions (after fine-tuning)\n",
        "def summarize_with_model(model, tokenizer, texts, max_length=150):\n",
        "    summaries = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        summary_ids = model.generate(inputs.input_ids, num_beams=4, max_length=max_length, early_stopping=True)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        summaries.append(summary)\n",
        "    return summaries\n",
        "\n",
        "# Summarize fine-tuned BART\n",
        "fine_tuned_bart_summaries = summarize_with_model(bart_model, bart_tokenizer, train_texts)\n",
        "\n",
        "# Summarize fine-tuned T5\n",
        "fine_tuned_t5_summaries = summarize_with_model(t5_model, t5_tokenizer, train_texts)\n",
        "\n",
        "# Summarize fine-tuned Pegasus\n",
        "fine_tuned_pegasus_summaries = summarize_with_model(pegasus_model, pegasus_tokenizer, train_texts)\n",
        "\n",
        "# Classification functions (after fine-tuning)\n",
        "def classify_with_bert(texts):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=1).numpy()\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Evaluate fine-tuned models on classification and summarization tasks\n",
        "def evaluate_summarization(original_texts, summaries):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    bleu_scores = []\n",
        "    for orig, sum_text in zip(original_texts, summaries):\n",
        "        rouge_scores = scorer.score(orig, sum_text)\n",
        "        bleu_scores.append(sentence_bleu([orig.split()], sum_text.split()))\n",
        "    return rouge_scores, bleu_scores\n",
        "\n",
        "def evaluate_classification(true_labels, predictions):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='weighted')\n",
        "    recall = recall_score(true_labels, predictions, average='weighted')\n",
        "    return accuracy, precision, recall\n",
        "\n",
        "# Evaluate summarization\n",
        "rouge_bart, bleu_bart = evaluate_summarization(train_texts, fine_tuned_bart_summaries)\n",
        "rouge_t5, bleu_t5 = evaluate_summarization(train_texts, fine_tuned_t5_summaries)\n",
        "rouge_pegasus, bleu_pegasus = evaluate_summarization(train_texts, fine_tuned_pegasus_summaries)\n",
        "\n",
        "# Evaluate classification\n",
        "bert_predictions = classify_with_bert(fine_tuned_bart_summaries)\n",
        "accuracy_bert, precision_bert, recall_bert = evaluate_classification(train_labels, bert_predictions)\n",
        "\n",
        "# Print results\n",
        "print(\"ROUGE Scores (Fine-tuned BART):\", rouge_bart)\n",
        "print(\"BLEU Scores (Fine-tuned BART):\", bleu_bart)\n",
        "print(\"Accuracy (Fine-tuned BERT):\", accuracy_bert)\n",
        "print(\"Precision (Fine-tuned BERT):\", precision_bert)\n",
        "print(\"Recall (Fine-tuned BERT):\", recall_bert)\n"
      ]
    }
  ]
}